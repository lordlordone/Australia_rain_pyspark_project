{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["HZ2b5z_gGNU0","Vl6rxzhNGCiu","clWe8qmkKX3W"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Data Preparation"],"metadata":{"id":"ngjYa80U3U9m"}},{"cell_type":"markdown","source":["## Creazione features Region, Month, Season\n"],"metadata":{"id":"skAPBp9pEcOC"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5hprk9T-GB1R","executionInfo":{"status":"ok","timestamp":1669672059933,"user_tz":-60,"elapsed":19174,"user":{"displayName":"Michele Andreucci","userId":"10752821942615030060"}},"outputId":"e63d0fef-cbee-45a2-90aa-2edaf75e0a86"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["!pip install pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZOJYuRDNGBt0","executionInfo":{"status":"ok","timestamp":1669672112553,"user_tz":-60,"elapsed":52625,"user":{"displayName":"Michele Andreucci","userId":"10752821942615030060"}},"outputId":"44034af0-614b-4ee7-d271-8792c135dc4f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark\n","  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n","\u001b[K     |████████████████████████████████| 281.4 MB 45 kB/s \n","\u001b[?25hCollecting py4j==0.10.9.5\n","  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n","\u001b[K     |████████████████████████████████| 199 kB 63.3 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=3a3da56a2f7d6f2afb05b99ed5d9de8a22ba194c317b43f21c8c724106bcf887\n","  Stored in directory: /root/.cache/pip/wheels/42/59/f5/79a5bf931714dcd201b26025347785f087370a10a3329a899c\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"]}]},{"cell_type":"code","source":["from pyspark.sql.functions import when, col, isnan, count,  regexp_replace, udf\n","import numpy as np\n","import pyspark\n","from pyspark.sql.types import FloatType, DoubleType\n","from pyspark.ml import Pipeline\n","from pyspark.ml.feature import VectorAssembler,MinMaxScaler, RobustScaler, StandardScaler, MaxAbsScaler, PCA\n","import pyspark.sql.functions as F\n","from pyspark.ml.clustering import KMeans, BisectingKMeansModel, KMeansSummary\n","from  pyspark.ml.classification import DecisionTreeClassifier\n","from pyspark.ml.evaluation import ClusteringEvaluator\n","from matplotlib.pyplot import figure\n","from pyspark.sql.functions import udf\n","from pyspark.sql.types import *\n","from yellowbrick.cluster import KElbowVisualizer\n","import matplotlib.pyplot as plt\n","from pyspark.sql import SQLContext\n","from pyspark.ml.feature import StringIndexer, VectorIndexer, StandardScaler, VectorAssembler\n","from pyspark.ml.classification import DecisionTreeClassifier\n","from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator, ClusteringEvaluator\n","from pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics\n","from pyspark.sql.functions import monotonically_increasing_id \n","from pyspark.ml.regression import LinearRegression\n","from pyspark.ml.regression import RandomForestRegressor\n","from pyspark.ml.evaluation import RegressionEvaluator"],"metadata":{"id":"P5VEarK8GBiH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark import SparkContext\n","# initialize a new Spark Context to use for the execution of the script\n","sc = SparkContext(appName=\"MY-APP-NAME\", master=\"local[*]\")"],"metadata":{"id":"t43l7LDSEfBJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql import SQLContext\n","sqlCtx = SQLContext(sc)"],"metadata":{"id":"P8NTkYqmEe5L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669672124973,"user_tz":-60,"elapsed":902,"user":{"displayName":"Michele Andreucci","userId":"10752821942615030060"}},"outputId":"593f7c13-3d70-4056-f9ee-35c47971aa1b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:114: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["# load the dataset\n","\n","rain_path = 'drive/MyDrive/DDAM/Australia Rain/australia_rain_tomorrow_raw.csv'"],"metadata":{"id":"_qgtF_jOGLeo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# carico il dataframe\n","df = sqlCtx.read.load(rain_path, format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")"],"metadata":{"id":"VVbR0KZSGLci","colab":{"base_uri":"https://localhost:8080/","height":328},"executionInfo":{"status":"error","timestamp":1669672130072,"user_tz":-60,"elapsed":5101,"user":{"displayName":"Michele Andreucci","userId":"10752821942615030060"}},"outputId":"36e9ca79-7f36-4c72-82d0-709418160564"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AnalysisException","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-6deea70b54f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# carico il dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlCtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/content/drive/MyDrive/DDAM/Australia Rain/australia_rain_tomorrow_raw.csv"]}]},{"cell_type":"code","source":["# Creazione colonna Month\n","df = df.withColumn(\"Month\", F.regexp_extract(col(\"Date\"), r'(\\d{1,2})/(\\d{1,2})/(\\d{4})', 2))\n","\n","# Creazione colonna Season\n","df = df.withColumn(\"Season\", .when((df.Month == 12.0) & (df.Month <=2.0),\"Winter\")\\\n","                               .when((df.Month > 2.0) & (df.Month <=5.0),\"Spring\") \\\n","                               .when((df.Month > 5.0) & (df.Month <=8.0),\"Summer\")\n","                                .otherwise(\"Fall\"))"],"metadata":{"id":"lPecoZrNGLZ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"**************Statistiche per variabile Season ********************\")\n","df.createOrReplaceTempView(\"tmp\")\n","tot = df.count()\n","query = sqlCtx.sql(\"SELECT Season, count(*)/\"+str(tot)+\" * 100 as percentage FROM tmp GROUP BY Season ORDER BY percentage DESC\")\n","query.show()"],"metadata":{"id":"ZnrdBsd9YfE_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"**************Statistiche per variabile Month ********************\")\n","tot = df.count()\n","query = sqlCtx.sql(\"SELECT Month, count(*)/\"+str(tot)+\" * 100 as percentage FROM tmp GROUP BY Month ORDER BY percentage DESC\")\n","query.show()"],"metadata":{"id":"wIQEojmrYlRL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = sqlCtx.sql(\"SELECT Month, RainToday, count(*) as cnt FROM tmp GROUP BY Month, RainToday ORDER BY cnt DESC\")\n","query.show(24)"],"metadata":{"id":"Rcbmz-d7YuCa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = sqlCtx.sql(\"SELECT Month, location, count(*) as cnt FROM tmp GROUP BY Month, location ORDER BY location, Month DESC\")\n","query.show(24)"],"metadata":{"id":"RCKwIP0Ben6L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = sqlCtx.sql(\"SELECT Date, location FROM tmp GROUP BY Date, location ORDER BY Date\")\n","query.show(24)"],"metadata":{"id":"8yL49vKge1nV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = sqlCtx.sql(\"SELECT Month, RainTomorrow, count(*) as cnt FROM tmp GROUP BY Month, RainTomorrow ORDER BY cnt DESC\")\n","query.show(25)"],"metadata":{"id":"DwC-tJMlZJcv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = sqlCtx.sql(\"SELECT Season, RainTomorrow, count(*) as cnt FROM tmp GROUP BY Season, RainTomorrow ORDER BY cnt DESC\")\n","query.show()"],"metadata":{"id":"hB46CDj8Zyap"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = sqlCtx.sql(\"SELECT Season, RainToday, count(*) as cnt FROM tmp GROUP BY Season, RainToday ORDER BY cnt DESC\")\n","query.show()"],"metadata":{"id":"vRxMSom0Z_sY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.show()"],"metadata":{"id":"KdACGuO2MPaW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creazione colonna Region\n","\n","# Percorso file con associazioni città-regione\n","filename_cities = 'drive/MyDrive/DDAM/Australia Rain/Mario/cities_australia.csv'\n","\n","# Creazione rdd (problemi usando il dataframe)\n","cities_rdd = sc.textFile(filename_cities)\n","cities_rdd.take(3)"],"metadata":{"id":"bhofRpyIGLXa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a dictionary (k: location, v: region)\n","\n","cities_rdd_dict = cities_rdd.map(lambda line: line.strip().split(\";\")).collectAsMap()\n","\n","cities_rdd_dict['Sydney']"],"metadata":{"id":"S1XZCMNnHqdE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["func_name = udf(\n","    lambda val: cities_rdd_dict[val], \n","    StringType()\n",")\n","\n","df = df.withColumn('Region', func_name(df.Location))"],"metadata":{"id":"hK1Q4BfKHyRo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.show(10)"],"metadata":{"id":"jSZjUifpH1If"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.groupBy('Region').count().show()"],"metadata":{"id":"VtuX_SekH553"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"**************Statistiche per variabile Region ********************\")\n","df.createOrReplaceTempView(\"tmp\")\n","tot = df.count()\n","query = sqlCtx.sql(\"SELECT Region, count(*)/\"+str(tot)+\" * 100 as percentage FROM tmp GROUP BY Region ORDER BY percentage DESC\")\n","query.show()"],"metadata":{"id":"qyTpQr5uXm2q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Tentativi di sostituzione missing values (groupBy)"],"metadata":{"id":"Jq8rTFPxETSo"}},{"cell_type":"code","source":["# Sostituisco 'NA' con null values \n","df = df.replace('NA', None)\n","df.show(5)"],"metadata":{"id":"Z8zbrEoREg3e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Colonne con più null values\n","\n","print('Totale osservazioni: 142193')\n","print()\n","\n","Dict_Null = {col:df.filter(df[col].isNull()).count() for col in df.columns}\n","Dict_Null = sorted(Dict_Null.items(), key=lambda x: x[1], reverse=True)\n","\n","for i in Dict_Null:\n","\tprint(i[0], i[1])"],"metadata":{"id":"jAAAiz9iFDDK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Analisi dei null values sulle colonne con più valori mancanti, raggruppati per città\n","\n","from pyspark.ml.stat import Correlation\n","\n","cols = [\"Evaporation\", \"Sunshine\", \"Cloud3pm\", \"Cloud9am\"]\n","\n","for c in cols:\n","    x = df.filter(df[c].isNull()).groupby(df.Location).count()\n","\n","    print('Null values for column {}'.format(c),'(grouped by city)')\n","    x.sort(\"count\", ascending=False).show(10)  # con print dà un None di troppo"],"metadata":{"id":"QTw9MfD5FCyE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Analisi dei null values sulle colonne con più valori mancanti, raggruppati per RainTomorrow\n","\n","cols = [\"Evaporation\", \"Sunshine\", \"Cloud3pm\", \"Cloud9am\"]\n","\n","for c in cols:\n","    x = df.filter(df[c].isNull()).groupby(df.RainTomorrow).count()\n","\n","    print('Null values for column {}'.format(c),'(grouped by RainTomorrow)')\n","    x.sort(\"count\", ascending=False).show(10)"],"metadata":{"id":"3S7bgyQFFCv7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Analisi dei null values sulle colonne con più valori mancanti, raggruppati per RainToday\n","# L'attributo RainToday, a differenza dei precedenti, presenta null values\n","\n","cols = [\"Evaporation\", \"Sunshine\", \"Cloud3pm\", \"Cloud9am\"]\n","\n","for c in cols:\n","    x = df.filter(df[c].isNull()).groupby(df.RainToday).count()\n","\n","    print('Null values for column {}'.format(c),'(grouped by RainToday)')\n","    x.sort(\"count\", ascending=False).show(10) "],"metadata":{"id":"DPeXE4FsFCtJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizzo la distribuzione di Yes e No in RainToday e RainTomorrow\n","\n","df.groupBy(\"RainToday\").count().show()\n","df.groupBy(\"RainTomorrow\").count().show()"],"metadata":{"id":"WSACsDvIFk8F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Media dell'attributo Evaporation (groupby RainToday e RainTomorrow)\n","\n","df.groupBy(\"RainToday\").agg({'Evaporation':'avg'}).show()\n","df.groupBy(\"RainTomorrow\").agg({'Evaporation':'avg'}).show()"],"metadata":{"id":"MqL31LD3Fk3B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Media dell'attributo Sunshine (groupby RainToday e RainTomorrow)\n","\n","df.groupBy(\"RainToday\").agg({'Sunshine':'avg'}).show()\n","df.groupBy(\"RainTomorrow\").agg({'Sunshine':'avg'}).show()"],"metadata":{"id":"4VOEVM52EguU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Media dell'attributo Cloud9am (groupby RainToday e RainTomorrow)\n","\n","df.groupBy(\"RainToday\").agg({'Cloud9am':'avg'}).show()\n","df.groupBy(\"RainTomorrow\").agg({'Cloud9am':'avg'}).show()"],"metadata":{"id":"vUdE6NxJFrs7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Media dell'attributo Cloud3pm (groupby RainToday e RainTomorrow)\n","\n","df.groupBy(\"RainToday\").agg({'Cloud3pm':'avg'}).show()\n","df.groupBy(\"RainTomorrow\").agg({'Cloud3pm':'avg'}).show()"],"metadata":{"id":"WD2MFJjbFrmX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### GroupBy Location"],"metadata":{"id":"HZ2b5z_gGNU0"}},{"cell_type":"code","source":["# Media dell'attributo Evaporation \n","# groupby Sunshine, ossia l'attributo con la correlazione più forte con Evaporation\n","# groupby Location\n","\n","df.groupBy(\"Location\",\"Sunshine\").agg({'Evaporation':'avg'}).sort('Location').show(5)"],"metadata":{"id":"CNFD4ndJFrg9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Media dell'attributo Sunshine \n","# groupby Evaporation, ossia l'attributo con la correlazione più forte con Sunshine\n","# groupby Location\n","\n","df.groupBy(\"Location\",\"Evaporation\").agg({'Sunshine':'avg'}).sort('Location').show(5)"],"metadata":{"id":"N8Ncgq57F1SY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Media dell'attributo Cloud9am \n","# groupby Humidity3pm, ossia l'attributo con la correlazione più forte con Cloud9am\n","# groupby Location\n","\n","df.groupBy(\"Location\",\"Humidity3pm\").agg({'Cloud9am':'avg'}).sort('Location', ascending=True).show(5)"],"metadata":{"id":"UxKN1jS-F1Ml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Media dell'attributo Cloud3pm \n","# groupby Humidity3pm, ossia l'attributo con la correlazione più forte con Cloud3pm\n","# groupby Location\n","\n","df.groupBy(\"Location\",\"Humidity3pm\").agg({'Cloud3pm':'avg'}).sort('Location', ascending=True).show(5)"],"metadata":{"id":"hnxa0A9SF79A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Per diverse città è impossibile fare la sostituzione dei null values con il groubby perché presentano troppi null values. Raggruppiamo quindi per l'attributo con la correlazione più forte e per periodo dell'anno."],"metadata":{"id":"8ifuBYS1F_WK"}},{"cell_type":"markdown","source":["#### GroupBy Month"],"metadata":{"id":"Vl6rxzhNGCiu"}},{"cell_type":"code","source":["df.groupBy(\"Month\").count().sort(\"Month\").show()"],"metadata":{"id":"zSvtEFVJF73k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Media dell'attributo Evaporation \n","# groupby Sunshine, ossia l'attributo con la correlazione più forte con Evaporation\n","# groupby Month\n","\n","df.groupBy(\"Month\",\"Sunshine\").agg({'Evaporation':'avg'}).sort('Month').show(5)"],"metadata":{"id":"MdtqIH1fGeuD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Media dell'attributo Sunshine \n","# groupby Evaporation, ossia l'attributo con la correlazione più forte con Sunshine\n","# groupby Date\n","\n","df.groupBy(\"Month\",\"Evaporation\").agg({'Sunshine':'avg'}).sort('Month').show(5)"],"metadata":{"id":"WGMe3sQBGeoR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Media dell'attributo Cloud9am \n","# groupby Humidity3pm, ossia l'attributo con la correlazione più forte con Cloud9am\n","# groupby Month\n","\n","df.groupBy(\"Month\",\"Humidity3pm\").agg({'Cloud9am':'avg'}).sort('Month').show(5)"],"metadata":{"id":"PxVeYccjGWd1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Media dell'attributo Cloud3pm \n","# groupby Humidity3pm, ossia l'attributo con la correlazione più forte con Cloud3pm\n","# groupby Month\n","\n","df.groupBy(\"Month\",\"Humidity3pm\").agg({'Cloud3pm':'avg'}).sort('Month').show(5)"],"metadata":{"id":"qy-JnQiDF7xF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sostituzione missing values con mediana"],"metadata":{"id":"pD_enkAZ3U3Q"}},{"cell_type":"code","source":["df = df.fillna('NA')\n","df.show(5)"],"metadata":{"id":"CGqaoeJQNucj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cols_to_impute = [\"Evaporation\", \"Sunshine\", \"Pressure9am\", \"Pressure3pm\", \"Cloud9am\", \"Cloud3pm\"]"],"metadata":{"id":"sPa9PxJC3dvy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# verifichiamo se a seconda di quando sta per piovere o se sta piovendo consecutivamente\n","# i valori possano seguire distribuzioni diverse\n","df = df.withColumn( 'it-will-rain' , when( (df.RainToday == 'No') & ( df.RainTomorrow == 'Yes'), 'Yes' ).otherwise('No') )   # oggi non piove e domani si\n","df = df.withColumn( 'no-rain' , when( (df.RainToday == 'No') & ( df.RainTomorrow == 'No'), 'Yes' ).otherwise('No') )         # oggi non piove e neanche domani\n","df = df.withColumn( 'it-rain' , when( (df.RainToday == 'Yes') & ( df.RainTomorrow == 'Yes'), 'Yes' ).otherwise('No') )       # oggi piove e pure domani\n","df = df.withColumn( 'end-rain' , when( (df.RainToday == 'Yes') & ( df.RainTomorrow == 'No'), 'Yes' ).otherwise('No') )       # oggi non piove e neanche domani\n","\n","df.show(5)"],"metadata":{"id":"y7fMELa8HO3s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_no_rain = df.where(df['no-rain']== 'Yes')\n","df_will_rain = df.where(df['it-will-rain'] == 'Yes')\n","df_it_rain = df.where(df['it-rain'] == 'Yes')\n","df_end_rain = df.where(df['end-rain'] == 'Yes')"],"metadata":{"id":"RxidovDbHOv_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# questa funzione mi serve per prendere un campione dell'attributo scelto\n","# che verra ritornato come array di numpy\n","def get_sample_(df, attr):\n","  df_filtered=df.filter(col(attr) != 'NA')\n","  sample = np.array( df_filtered.select(col(attr)).rdd.takeSample(False, 3000, seed= 42) ).flatten()\n","  return sample"],"metadata":{"id":"T49oOLctEhTY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def check_distribution(attr):\n","  # ottengo un campione dei dati dell'attributo specificato\n","  sample_no_rain = get_sample_(df_no_rain, attr).astype(np.float)\n","  sample_will_rain = get_sample_(df_will_rain, attr).astype(np.float)\n","  sample_rain= get_sample_(df_it_rain, attr).astype(np.float)\n","  sample_end_rain = get_sample_(df_end_rain, attr).astype(np.float)\n","\n","  # faccio un test per vedere se seguono la stessa distribuzione\n","  from scipy.stats import median_test\n","\n","  alfa = 0.01    # alfa e il livello di confidenza con il quale vogliamo essere sicuri se accettare o \n","                # rifiutare l'ipotesi nulla\n","\n","  print(\"Median Test for attribute <\"+ attr + \">\")\n","  stat, p, med, tbl = median_test(sample_no_rain, sample_will_rain)\n","  print('   no-rain and will-rain ',p < alfa)\n","\n","  stat, p, med, tbl = median_test(sample_will_rain, sample_rain)\n","  print('   will-rain and it-rain ',p < alfa)\n","\n","  stat, p, med, tbl = median_test(sample_rain, sample_end_rain)\n","  print('   it-rain and end-rain', p < alfa)\n","  print(\"\")"],"metadata":{"id":"2fxyyyQaHv10"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for attr in cols_to_impute:\n","  check_distribution(attr)"],"metadata":{"id":"W314IeGeHvw3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_median(df, attr):\n","  # pulisco dai NA values\n","  df_filtered= df.filter(col(attr) != 'NA')\n","  # converto la colonna in float\n","  df_float = df_filtered.withColumn(attr, col(attr).cast(FloatType()))\n","  return df_float.approxQuantile(attr, [0.5], 0.25)  # ritorna la mediana dell'attributo scelto\n","\n","\n","def get_median_(df, attr, dict_median):\n","  # pulisco dai NA values\n","  df_filtered= df.filter(col(attr) != 'NA')\n","  # converto la colonna in float\n","  df_float = df_filtered.withColumn(attr, col(attr).cast(FloatType()))\n","  median = df_float.approxQuantile(attr, [0.5], 0.25)\n","  dict_median[attr] = median\n","  return dict_median "],"metadata":{"id":"r2IVmLs3HvrZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sostituiamo i valori mancanti con la mediana\n","dict_median = {}\n","for attr in cols_to_impute:\n","  if attr!=\"Evaporation\":\n","    dict_median = get_median_(df,attr, dict_median)\n","    median_attr = str(dict_median[attr][0])\n","    df = df.withColumn(attr, regexp_replace(attr, 'NA', median_attr)).withColumn(attr, col(attr).cast(FloatType()))"],"metadata":{"id":"JjCbAlQbHvjs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Poiché no-rain, will-rain e it-rain hanno dei p-value molto piccoli possiamo assumere che abbiano la stessa mediana, ma per quanto riguarda it-rain e end-rain, non seguono la stessa mediana"],"metadata":{"id":"DLPl3NqTIL3U"}},{"cell_type":"code","source":["# imputiamo quindi i valori, ricordandoci che siccome\n","# no-rain, will-rain e it-rain hanno la stessa mediana, per loro \n","# sara identica, mentre lo stesso non avverra per end-rain\n","\n","median_rain = get_median(df.where(col('end-rain') == 'No' ), 'Evaporation')\n","median_rain= str(median_rain[0])\n","\n","median_noRain = get_median(df.where(col('end-rain') == 'Yes'), 'Evaporation')\n","median_noRain= str(median_noRain[0])\n","\n","# sostituiamo i valori mancanti con la mediana\n","df = df.withColumn('Evaporation', when( (col('end-rain') == 'No') & (col('Evaporation') == 'NA') , regexp_replace('Evaporation', 'NA', median_rain))\\\n","                            .otherwise( when(   (col('end-rain') == 'Yes') & (col('Evaporation') == 'NA') , regexp_replace('Evaporation', 'NA', median_noRain))\\\n","                            .otherwise( col('Evaporation')))).withColumn('Evaporation', col('Evaporation').cast(FloatType()))"],"metadata":{"id":"JFRl81QIIMTP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for attr in cols_to_impute:\n","  df.select([count(when(isnan(attr),True))]).show()"],"metadata":{"id":"rCVcFEyeIWKe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = df.drop(\"it-will-rain\", \"no-rain\", \"it-rain\", \"end-rain\")\n","df.show(10)"],"metadata":{"id":"IREkDOcLIfcG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cat_cols = [\"Date\", \"Location\", \"WindGustDir\", \"WindDir9am\", \"WindDir3pm\",\n","            \"RainTomorrow\", \"RainToday\", \"Evaporation\", \"Sunshine\", \"Pressure9am\",\n","            \"Pressure3pm\", \"Cloud9am\", \"Cloud3pm\", 'Month', 'Season', 'Region']\n","\n","col_to_float = [i for i in df.columns if i not in cat_cols]"],"metadata":{"id":"U2KZtpvPI5ml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["col_to_float"],"metadata":{"id":"-QDUzkRVj_E1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for attr in col_to_float:\n","  df = df.withColumn(attr, col(attr).cast(FloatType()))"],"metadata":{"id":"be2q7LLtI5f6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.show()"],"metadata":{"id":"-AbJhRvbOWBn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"xHbfM9fmj3CU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dIIfMAepoak4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Salvataggio nuovo dataframe 'rain_tomorrow_australia'\n","\n","df.toPandas().to_csv('drive/MyDrive/DDAM/Australia Rain/rain_tomorrow_australia_PROVA.csv', header=True)"],"metadata":{"id":"MURF-wyyIWBg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Confronto tra mediana e regressione per la sostituzione dei missing values"],"metadata":{"id":"JiTyTtMe3Uyq"}},{"cell_type":"code","source":["# Caricamento nuovo dataset senza missing values\n","\n","filename = 'drive/MyDrive/DDAM/Australia Rain/rain_tomorrow_australia.csv'\n","df_median = sqlCtx.read.load(filename, format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n","\n","# Caricamento dataset originale\n","\n","filename2 = 'drive/MyDrive/DDAM/Australia Rain/australia_rain_tomorrow_raw.csv'\n","df_original = sqlCtx.read.load(filename2, format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")"],"metadata":{"id":"qwNxnKXJJ2x-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Prendo un campione di osservazioni del dataframe. Per capire quali righe non avevano missing values utilizzo direttamente il vecchio dataframe."],"metadata":{"id":"jKvaongRJv7f"}},{"cell_type":"code","source":["# Creo una colonna id nel dataframe originale\n","df_original = df_original.select(\"*\").withColumn(\"_c0\", monotonically_increasing_id())\n","\n","# Riordino le colonne\n","df_original = df_original.select('_c0','Date','Location','MinTemp','MaxTemp','Rainfall','Evaporation','Sunshine',\n"," 'WindGustDir','WindGustSpeed','WindDir9am','WindDir3pm','WindSpeed9am','WindSpeed3pm','Humidity9am',\n"," 'Humidity3pm','Pressure9am','Pressure3pm','Cloud9am','Cloud3pm','Temp9am','Temp3pm','RainToday','RISK_MM',\n"," 'RainTomorrow') "],"metadata":{"id":"FUQJxdFZ3jZI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Nel nuovo dataframe sono stati sostituiti i missing values per le colonne Evaporation, Sunshine, Cloud9am, Cloud3pm, Pressure9am, Pressure3pm. \n","\n","Per fare la predizione con il vecchio dataframe devo escludere tutte queste colonne (se i valori sono stati sostituiti è perché erano missing values -> impossibile fare regressione).\n","\n","Tengo tuttavia tutti i valori di Sunshine, per cui devo fare la predizione."],"metadata":{"id":"ljppMAX5KGu4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pM6sW1pR3S0Q"},"outputs":[],"source":["cols = ['_c0','Sunshine','MinTemp','MaxTemp','Rainfall','WindGustSpeed','WindSpeed9am',\n","        'WindSpeed3pm','Humidity9am','Humidity3pm','Temp3pm']\n","\n","df_original = df_original.select(*cols)\n","df_median = df_median.select(*cols)"]},{"cell_type":"code","source":["# Rimuovo osservazioni con missing values (ad esclusione di Sunshine) dal df originale\n","\n","df_original = df_original.replace('NA', None)\n","df_original = df_original.na.drop(subset=['_c0','MinTemp','MaxTemp','Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm','Humidity9am','Humidity3pm','Temp3pm'])"],"metadata":{"id":"X--uY03NJcqQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Totale osservazioni:', df_original.count())\n","print()\n","print('Null values per column:')\n","\n","Dict_Null = {col:df_original.filter(df_original[col].isNull()).count() for col in df_original.columns}\n","Dict_Null = sorted(Dict_Null.items(), key=lambda x: x[1], reverse=True)\n","\n","for i in Dict_Null:\n","\tprint(i[0], i[1])"],"metadata":{"id":"ZTvmBZ6KJcne"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Del nuovo dataset tengo solo le righe che nel vecchio dataset non avevano null values (escluso Sunshine)\n","# Faccio una join sulla colonna id\n","\n","df_median = df_median.join(df_original, df_median._c0 == df_original._c0, \"semi\")\n","\n","df_median.join(df_original, df_median._c0 == df_original._c0, 'semi').show(5)\n","df_median.join(df_original, df_median._c0 == df_original._c0, 'semi').count()\n","\n","df_median.filter(df_median['Sunshine'].isNull()).count()\n","\n","print('N° osservazioni rimaste', df_median.count())"],"metadata":{"id":"xKtEjcoSJckp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Regressione per Sunshine\n","\n","Faccio la regressione per Sunshine. Ottengo una colonna di predizioni e la confronto con la colonna Sunshine del nuovo dataset."],"metadata":{"id":"clWe8qmkKX3W"}},{"cell_type":"code","source":["# Seleziono colonne\n","df_regression = df_median\n","\n","# Trasformo in float\n","df_regression = df_regression.select(*(col(c).cast(\"float\").alias(c) for c in df_regression.columns))"],"metadata":{"id":"GPIzMMd8Ka0s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_regression.printSchema()"],"metadata":{"id":"sRrr33Y-Kayk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creo il vettore di valori\n","\n","in_cols= [col for col in df_regression.columns if col != 'Sunshine']\n","\n","vecAssembler = VectorAssembler(inputCols=in_cols, outputCol=\"features\")\n","vec_df = vecAssembler.transform(df_regression)\n","vec_df = vec_df.select(['features', 'Sunshine'])\n","vec_df.show(3)"],"metadata":{"id":"14_ywW_XJcg8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vec_df.printSchema()"],"metadata":{"id":"nR9uOCRZJcdQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creo modello e ottengo le predizioni\n","\n","lr = LinearRegression(featuresCol = 'features', labelCol='Sunshine')\n","lr_model = lr.fit(vec_df)\n","lr_predictions = lr_model.transform(vec_df)\n","\n","print('N° predizioni:',lr_predictions.count())"],"metadata":{"id":"uFBxp80LKqXz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr_predictions.show(5)\n","lr_predictions.printSchema()"],"metadata":{"id":"D166lnHbKtoZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Evaluation\n","\n","Ora abbiamo la colonna Sunshine e la colonna con le predizioni. Misuriamo quanto sono diverse."],"metadata":{"id":"GUh1cCzTK8y7"}},{"cell_type":"code","source":["lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"Sunshine\",metricName=\"r2\")\n","lr_evaluator2 = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"Sunshine\",metricName=\"rmse\")\n","\n","print(\"R Squared = %g\" % lr_evaluator.evaluate(lr_predictions))\n","print(\"RMSE = %g\" % lr_evaluator2.evaluate(lr_predictions))"],"metadata":{"id":"NYGMtXmCK6vi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aggiungo la colonna differenza (in valore assoluto)\n","from  pyspark.sql.functions import abs\n","\n","lr_predictions = lr_predictions.withColumn('difference', abs(lr_predictions.Sunshine - lr_predictions.prediction))\n","\n","lr_predictions.show(30)"],"metadata":{"id":"j7-9EGRRLD2m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr_predictions.select(lr_predictions['difference']).summary().show()"],"metadata":{"id":"MX7HE7jELFo-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pG-iXybVLIOx"},"execution_count":null,"outputs":[]}]}